## 介绍

要搞明白 Go 语言的内存管理，就必须先理解操作系统以及机器硬件是如何管理内存的。因为 Go 语言的内部机制是建立在这个基础之上的，它的设计，本质上就是尽可能的会发挥操作系统层面的优势，而避开导致低效情况。

## 操作系统内存管理

其实现在计算机内存管理的方式都是一步步演变来的，最开始是非常简单的，后来为了满足各种需求而增加了各种各样的机制，越来越复杂。这里我们只介绍和开发者息息相关的几个机制。

## 最原始的方式

我们可以把内存看成一个数组，每个数组元素的大小是 1B，也就是 8 位(bit)。CPU 通过内存地址来获取内存中的数据，内存地址可以看做成数组的游标（index）。

![img](https://cdn.nlark.com/yuque/0/2019/jpeg/106947/1568950565136-857c3752-8114-4097-b001-8905501985f8.jpeg)

CPU 在执行指令的时候，就是通过内存地址，将物理内存上的数据载入到寄存器，然后执行机器指令。但随着发展，出现了多任务的需求，也就是希望多个任务能同时在系统上运行。这就出现了一些问题：

1、内存访问冲突:程序很容易出现 bug，就是 2 或更多的程序使用了同一块内存空间，导致数据读写错乱，程序崩溃。更有一些黑客利用这个缺陷来制作病毒。

2、内存不够用：因为每个程序都需要自己单独使用的一块内存，内存的大小就成了任务数量的瓶颈。

3、程序开发成本高：**你的程序要使用多少内存，内存地址是多少，这些都不能搞错，对于人来说，开发正确的程序很费脑子。

举个例子，假设有一个程序，当代码运行到某处时，需要使用 100M 内存，其他时候 1M 内存就够；为了避免和其他程序冲突，程序初始化时，就必须申请独立 100M 内存以保证正常运行，这就是一种很大的浪费，因为这 100M 它大多数时候用不上，其他程序还不能用。

## 虚拟内存

虚拟内存的出现，很好的为了解决上述的一些列问题。用户程序只能使用虚拟的内存地址来获取数据，系统会将这个虚拟地址翻译成实际的物理地址。 所有程序统一使用一套连续虚拟地址，比如 0x0000 ~ 0xffff。从程序的角度来看，它觉得自己独享了一整块内存。不用考虑访问冲突的问题。系统会将虚拟地址翻译成物理地址，从内存上加载数据。 对于内存不够用的问题，虚拟内存本质上是将磁盘当成最终存储，而主存作为了一个 cache。程序可以从虚拟内存上申请很大的空间使用，比如 1G ；但操作系统不会真的在物理内存上开辟 1G 的空间，它只是开辟了很小一块，比如 1M 给程序使用。 这样程序在访问内存时，操作系统看访问的地址是否能转换成物理内存地址。能则正常访问，不能则再开辟。这使得内存得到了更高效的利用。 如下图所示，每个进程所使用的虚拟地址空间都是一样的，但他们的虚拟地址会被映射到主存上的不同区域，甚至映射到磁盘上（当内存不够用时）。

![img](https://cdn.nlark.com/yuque/0/2019/jpeg/106947/1568950565174-208ec960-87ad-4db3-841d-9d9a9908d23f.jpeg)

**虚拟地址**

其实本质上很简单，就是操作系统将程序常用的数据放到内存里加速访问，不常用的数据放在磁盘上。这一切对用户程序来说完全是透明的，用户程序可以假装所有数据都在内存里，然后通过虚拟内存地址去访问数据。在这背后，操作系统会自动将数据在主存和磁盘之间进行交换。

## 虚拟地址翻译

虚拟内存的实现方式，大多数都是通过页表来实现的。操作系统虚拟内存空间分成一页一页的来管理，每页的大小为 4K （当然这是可以配置的，不同操作系统不一样）。磁盘和主内存之间的置换也是以页为单位来操作的。4K 算是通过实践折中出来的通用值，太小了会出现频繁的置换，太大了又浪费内存。  虚拟地址 -> 物理地址 的映射关系由**页表（Page Table）**记录，它其实就是一个数组，数组中每个元素叫做**页表条目（Page Table Entry，简称 PTE）**，PTE 由一个有效位和 n 位地址字段构成，有效位标识这个虚拟地址是否分配了物理内存。 页表被操作系统放在物理内存的指定位置，CPU  上有个 Memory Management Unit（MMU） 单元，CPU 把虚拟地址给 MMU，MMU 去物理内存中查询页表，得到实际的物理地址。当然 MMU 不会每次都去查的，它自己也有一份缓存叫Translation Lookaside Buffer (TLB)，是为了加速地址翻译。

![img](https://cdn.nlark.com/yuque/0/2019/jpeg/106947/1568950565174-5a330cf2-19d0-4e7f-9eeb-51df5a8843ee.jpeg)

**虚拟地址翻译**

**你慢慢会发现整个计算机体系里面，缓存是无处不在的，整个计算机体系就是建立在一级级的缓存之上的，无论软硬件。**

让我们来看一下 CPU 内存访问的完整过程：

CPU 使用虚拟地址访问数据，比如执行了 MOV 指令加载数据到寄存器，把地址传递给 MMU。 MMU 生成 PTE 地址，并从主存（或自己的 Cache）中得到它。 如果 MMU 根据 PTE 得到真实的物理地址，正常读取数据。流程到此结束。 如果 PTE 信息表示没有关联的物理地址，MMU 则触发一个缺页异常。 操作系统捕获到这个异常，开始执行异常处理程序。在物理内存上创建一页内存，并更新页表。 缺页处理程序在物理内存中确定一个**牺牲页**，如果这个牺牲页上有数据，则把数据保存到磁盘上。 缺页处理程序更新 PTE。 缺页处理程序结束，再回去执行上一条指令（导致缺页异常的那个指令，也就是 MOV 指令）。这次肯定命中了。

## 内存命中率

你可能已经发现，上述的访问步骤中，从第 4 步开始都是些很繁琐的操作，频繁的执行对性能影响很大。毕竟访问磁盘是非常慢的，它会引发程序性能的急剧下降。如果内存访问到第 3 步成功结束了，我们就说**页命中**了；反之就是**未命中**，或者说**缺页**，表示它开始执行第 4 步了。

假设在 n 次内存访问中，出现命中的次数是 m，那么 m / n * 100% 就表示命中率，这是衡量内存管理程序好坏的一个很重要的指标。

如果物理内存不足了，数据会在主存和磁盘之间频繁交换，命中率很低，性能出现急剧下降，我们称这种现象叫**内存颠簸**。这时你会发现系统的 swap 空间利用率开始增高， CPU 利用率中 iowait 占比开始增高。

大多数情况下，只要物理内存够用，页命中率不会非常低，不会出现内存颠簸的情况。因为大多数程序都有一个特点，就是**局部性**。

**局部性就是说被引用过一次的存储器位置，很可能在后续再被引用多次；而且在该位置附近的其他位置，也很可能会在后续一段时间内被引用。**

前面说过计算机到处使用一级级的缓存来提升性能，归根结底就是利用了局部性的特征，如果没有这个特性，一级级的缓存不会有那么大的作用。所以一个局部性很好的程序运行速度会更快。

## CPU Cache

随着技术发展，CPU 的运算速度越来越快，但内存访问的速度却一直没什么突破。最终导致了 CPU 访问主存就成了整个机器的性能瓶颈。CPU Cache 的出现就是为了解决这个问题，在 CPU 和 主存之间再加了 Cache，用来缓存一块内存中的数据，而且还不只一个，现代计算机一般都有 3 级 Cache，其中 L1 Cache 的访问速度和寄存器差不多。

现在访问数据的大致的顺序是 CPU --> L1 Cache --> L2 Cache --> L3 Cache --> 主存 --> 磁盘 。从左到右，访问速度越来越慢，空间越来越大，单位空间（比如每字节）的价格越来越低。

现在存储器的整体层次结构大致如下图：

![img](https://cdn.nlark.com/yuque/0/2019/jpeg/106947/1568950565195-3231f469-1ed6-41d6-b947-1261d6c6cd24.jpeg)

**存储器层次结构**

在这种架构下，缓存的命中率就更加重要了，因为系统会假定所有程序都是有局部性特征的。如果某一级出现了未命中，他就会将该级存储的数据更新成最近使用的数据。

主存与存储器之间以 page（通常是 4K） 为单位进行交换，cache 与 主存之间是以 cache line（通常 64 byte） 为单位交换的。

## 举个例子

让我们通过一个例子来验证下命中率的问题，下面的函数是循环一个数组为每个元素赋值。

```go
func Loop(nums []int, step int){

   l :=len(nums)

	for i :=0; i < step; i++{

		for j := i; j < l; j += step {

      	nums[j]=4

		}
	}
}
```

参数 step 为 1 时，和普通一层循环一样。假设 step 为 2 ，则效果就是跳跃式遍历数组，如 1,3,5,7,9,2,4,6,8,10 这样，step 越大，访问跨度也就越大，程序的局部性也就越不好。 下面是 nums 长度为 10000 ， step = 1 和 step = 16 时的压测结果：

```
goos: darwin

goarch: amd64

BenchmarkLoopStep1-43000005241 ns/op

BenchmarkLoopStep16-410000022670 ns/op
```

可以看出，2 种遍历方式会出现 3 倍的性能差距。这种问题最容易出现在多维数组的处理上，比如遍历一个二维数组很容易就写出局部性很差的代码。

## 程序的内存布局

最后看一下程序的内存布局。现在我们知道了每个程序都有自己一套独立的地址空间可以使用，比如 0x0000 ~ 0xffff ，但我们在用高级语言，无论是 C 还是 Go 写程序的时候，很少直接使用这些地址。我们都是通过变量名来访问数据的，编译器会自动将我们的变量名转换成真正的虚拟地址。

那最终编译出来的二进制文件，是如何被操作系统加载到内存中并执行的呢？

其实，操作系统已经将一整块内存划分好了区域，每个区域用来做不同的事情。如图：

![img](https://cdn.nlark.com/yuque/0/2019/jpeg/106947/1568950565204-254bd747-d90e-4782-b1c5-7e07911fe317.jpeg)

**内存布局**

**text 段**：存储程序的二进制指令，及其他的一些静态内容

**data 段**：用来存储已被初始化的全局变量。比如常量（const）。

**bss 段**：用来存放未被初始化的全局变量。和 .data 段一样都属于静态分配，在这里面的变量数据在编译就确定了大小，不释放。

**stack 段**：栈空间，主要用于函数调用时存储临时变量的。这部分的内存是自动分配自动释放的。

**heap 段**：堆空间，用于动态分配，C 语言中 malloc 和 free 操作的内存就在这里；Go 语言主要靠 GC 自动管理这部分。

其实现在的操作系统，进程内部的内存区域没这么简单，要比这复杂多了，比如内核区域，共享库区域。因为我们不是要真的开发一套操作系统，细节可以忽略。这里只需要记住**堆空间**和**栈空间**即可。

**栈空间**是通过压栈出栈方式自动分配释放的，由系统管理，使用起来高效无感知。

**堆空间**是用以动态分配的，由程序自己管理分配和释放。Go 语言虽然可以帮我们自动管理分配和释放，但是代价也是很高的。

## 结论

局部性好的程序，可以提高缓存命中率，这对底层系统的内存管理是很友好的，可以提高程序的性能。CPU Cache 层面的低命中率导致的是程序运行缓慢，内存层面的低命中率会出现内存颠簸，出现这种现象时你的服务基本上已经瘫痪了。Go 语言的内存管理是参考 tcmalloc 实现的，它其实就是利用好了 OS 管理内存的这些特点，来最大化内存分配性能的。

## 参考

1、Go Memory Management

2、《深入理解计算机系统》
